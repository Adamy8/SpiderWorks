Scraping:
    py - BeautifulSoup
    js - xxos
    pip - Scrapy  at  https://scrapy.org/

python3 -m venv name_of_venv
--Activate venv
source venv/bin/activate
--To deactivate venv
deactivate
--When making requirements file
pip3 freeze > requirements.txt

MySQL:
    mysql -u root -p        // to enter
    pip install mysql mysql-connector-python

MySQL cmd:
    show databases;
    use books;   // use any database
    show tables;
    SELECT * FROM books LIMIT 2;    // books is one of the tables
    drop table books;   //remove the table

Scrapy cmd:
    pip install scrapy
    scrapy startproject bookscraper
    scrapy genspider bookspider books.toscrape.com
    pip install ipython         // for scrapy shell
    scrapy shell , quit() or exit()

    scrapy crawl bookspider -O bookdata.csv           // -O for new write, -o for append
    scrapy crawl bookspider -O bookdata.json

User agent:
    -info: https://useragentstring.com/
    -ulimited api call to ScrapeOps for fake headers: https://scrapeops.io/app/headers

Rotating proxies: (I did NOT implement M2 & M3)
M1:
    https://github.com/TeamHG-Memex/scrapy-rotating-proxies
    pip install scrapy-rotating-proxies
    get free proxies at: https://geonode.com/free-proxy-list
M2:
    service by https://smartproxy.com/ (paid, residential/datacenter)
    It gives a url that generates proxies that accroding to the requirements(country)
    to use it: put in yield response, meta={"proxy": " the url "}, or use middleware
M3:
    use ScrapeOps service: proxy aggregator @ https://scrapeops.io/app/proxy
    It handles all rotation, just simply calls its url with the scraping destnation included.
    use function start_requests from scrapy -> to enable the first request to start with that url instead of start_urls
    *remember to add "proxy.scrapeops.io" to allowed_domains
    pip install scrapeops-scrapy-proxy-sdk
        -> run the proxy through middleware(no need to change the request url, the middleware will handle the url change)
        -> simplify (only a few line need to add in settings.py)
        -> many other feature available:eg. SCRAPEOPS_PROXY_SETTINGS = {'country': 'us'}


Robots.txt:
    in settings.py: set it to false to not follow the rules

xpath & css selector:
    both reture data from DOM, xpath is stronger, css -> easier to use

SPIDER_MIDDLEWARES vs DOWNLOADER_MIDDLEWARES:
    **Spider middlewares** handle the data after it is downloaded and passed to the spider, allowing you to process or modify the responses and items. **Downloader middlewares** manage requests before they are sent and responses after they are received, allowing you to modify headers, rotate proxies, handle retries, and more. Both types of middlewares work together to manage the flow of requests and data in Scrapy.

Deploy:
1, ScrapyD (free) opensource, need 3rd party server (part10)
    3rd party server service: DigitalOcean(server provider, use to set up VMs) / VULTR / AWS
    Use ubantu(VM) on a 3rd party server, clone the repo, and run just like in bash.(some config need to be done)
    pip install some libraries for Scrapyd. Two UI option for handling spiders: ScrapydWeb / ScrapyOps Monitoring service
2, ScrapeOps Server&Deployment service (free,not really) (part11)
    same with last one, start with a server that provides VM, could be DigitalOcean/AWS/VULTR
    can conle the repo directly for deployment, add deploy key in github
    very easy to deploy compared to 1, UI: can use the same ScrapeOps Monitoring service set up.(follow the docs)
3, ScrapyCloud (paid) (part12)
    

5h course:
    https://www.youtube.com/watch?v=mBoX_JCKZTE